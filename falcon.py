
import os
import pandas as pd
import streamlit as st
import requests
import traceback

# =========================
# CONFIGURACI√ìN DE LA P√ÅGINA
# =========================
st.set_page_config(
    page_title="Generador de Poemas con IA",
    page_icon="‚úçÔ∏è",
    layout="wide",
)

# =========================
# DIAGN√ìSTICO Y CARGA INICIAL
# =========================
csv_path = "poems_clean.csv"
df = None
try:
    df = pd.read_csv(csv_path)
except Exception:
    st.sidebar.error("Error: No se pudo cargar poems_clean.csv. Verifica que est√© en la ra√≠z.")
    df = None

HF_TOKEN = os.getenv("HF_TOKEN") or st.secrets.get("HF_TOKEN")
if not HF_TOKEN:
    st.sidebar.warning("‚ö†Ô∏è HF_TOKEN no encontrado. Config√∫ralo como variable de entorno o en st.secrets['HF_TOKEN'].")

# =========================
# CONFIGURACI√ìN DEL MODELO Y ROUTER
# =========================
MODEL_ID = "HuggingFaceH4/zephyr-7b-beta"  # principal
FALLBACK_MODEL_ID = "gpt2"                  # respaldo

def router_url(model_id: str) -> str:
    return f"https://router.huggingface.co/models/{model_id}"

def hf_generate(prompt, model_id=MODEL_ID, max_tokens=300, temperature=0.9, return_full_text=False):
    """
    Cliente HTTP para Hugging Face Router con manejo de errores y fallback.
    - Payload: 'inputs' + 'parameters' (kwargs de pipelines).
    """
    headers = {
        "Authorization": f"Bearer {HF_TOKEN}",
        "Accept": "application/json",
        "Content-Type": "application/json",
    }
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": max_tokens,
            "temperature": temperature,
            "return_full_text": return_full_text,
            # opcionales: top_p, top_k, repetition_penalty, etc.
        }
    }

    try:
        resp = requests.post(router_url(model_id), headers=headers, json=payload, timeout=180)
        resp.raise_for_status()
        data = resp.json()

        # El Router mantiene 'generated_text' para text-generation
        if isinstance(data, list) and data and isinstance(data[0], dict):
            if "generated_text" in data[0]:
                return data[0]["generated_text"], model_id, False
            # intento gen√©rico por si el proveedor devuelve clave distinta
            for k in ("text", "output_text"):
                if k in data[0]:
                    return data[0][k], model_id, False

        elif isinstance(data, dict) and "generated_text" in data:
            return data["generated_text"], model_id, False

        return "Error: Respuesta inesperada del Router.", model_id, False

    except requests.HTTPError as e:
        status_code = e.response.status_code

        # 404: el modelo no est√° disponible -> fallback
        if status_code == 404 and model_id != FALLBACK_MODEL_ID:
            # Avisamos que activamos fallback
            st.info(f"‚ÑπÔ∏è 404 con **{model_id}**. Cambiando a modelo de respaldo: **{FALLBACK_MODEL_ID}**.")
            return hf_generate(prompt, model_id=FALLBACK_MODEL_ID,
                               max_tokens=max_tokens, temperature=temperature,
                               return_full_text=return_full_text)

        # 503: servicio no disponible (cold start)
        if status_code == 503:
            return "üíî **503**: el modelo est√° cargando o no acepta tr√°fico ahora (Cold Start).", model_id, False

        # 410: si alguna llamada antigua llega a api-inference (deprecado)
        if status_code == 410:
            return ("‚ö†Ô∏è El endpoint api-inference.huggingface.co fue deprecado. "
                    "Usa https://router.huggingface.co/models/{model_id}."), model_id, False

        # Otros errores HTTP
        return f"üö® Error HTTP del Router: {status_code} - {e.response.text}", model_id, False

    except requests.exceptions.Timeout:
        return "‚è∞ **Timeout**: el modelo tard√≥ demasiado en responder.", model_id, False
    except Exception as e:
        return "üö® Error inesperado durante la generaci√≥n.\n" + "".join(traceback.format_exception(e)), model_id, False

# =========================
# INTERFAZ STREAMLIT
# =========================
st.title("‚úçÔ∏è IA Generativa de Poemas en Espa√±ol")

st.markdown(f"""
Aplicaci√≥n generativa de poemas en espa√±ol usando **{MODEL_ID}** (v√≠a Hugging Face Router).
Si no est√° disponible, se aplicar√° fallback autom√°tico a **{FALLBACK_MODEL_ID}**.
""")

st.subheader("Configuraci√≥n de la Generaci√≥n")

col1, col2 = st.columns(2)
with col1:
    tema = st.text_input("Tema del poema", placeholder="Ej: La melancol√≠a del oto√±o")
with col2:
    estilo = st.selectbox(
        "Estilo",
        ["Verso libre", "Soneto", "Haiku", "Romance", "D√©cima", "Oda",
         "Copla", "Eleg√≠a", "√âgloga", "Lira", "Redondilla"]
    )

if st.button("‚ú® Generar Poema", type="primary"):
    if not tema or len(tema.strip()) < 3:
        st.error("Por favor, ingresa un tema v√°lido para la generaci√≥n.")
    elif not HF_TOKEN:
        st.error("El token de Hugging Face (HF_TOKEN) es necesario.")
    elif df is None or df.empty:
        st.error("El dataset de poemas no se carg√≥ correctamente.")
    else:
        # 1) Preparar ejemplos y prompt
        ejemplos = df['content'].dropna().sample(min(3, len(df))).tolist()
        ejemplos_texto = "\n".join([f"- {e.strip()[:200]}..." for e in ejemplos])

        prompt = f"""
Eres un poeta experto en espa√±ol.
Escribe un poema sobre el tema: "{tema}".
Estilo: {estilo}.
Insp√≠rate en el estilo (sin copiar) de estos ejemplos:
{ejemplos_texto}

Ahora escribe el poema:
""".strip()

        # 2) Generar con spinner
        st.subheader(f"Resultado: Poema '{estilo}' sobre '{tema}'")
        with st.spinner("‚è≥ La IA est√° escribiendo... Esto puede tardar varios segundos."):
            poem, used_model, used_fallback_flag = hf_generate(
                prompt,
                model_id=MODEL_ID,
                max_tokens=300,
                temperature=0.9,
                return_full_text=False
            )

        # 3) Mostrar resultado y avisos
        st.success(f"‚úÖ Generaci√≥n completada con **{used_model}**.")
        if used_model == FALLBACK_MODEL_ID:
            st.warning("üîÅ Se activ√≥ el **fallback** al modelo de respaldo (gpt2) porque el principal no estaba disponible.")

        st.markdown("---")
        st.markdown(poem)
        st.markdown("---")

# =========================
# AYUDA SOBRE ESTILOS
# =========================
st.markdown("""
---
### Estilos Disponibles:
* **Verso libre**: Poema sin rima ni m√©trica fija.
* **Soneto**: 14 versos endecas√≠labos con rima organizada.
* **Haiku**: Tres versos breves inspirados en la naturaleza.
* **Romance**: Versos octos√≠labos con rima asonante en pares.
* **D√©cima**: 10 versos octos√≠labos con rima ABBAACCDDC.
* **Oda**: Poema solemne y reflexivo.
* **Copla**: Estrofa de 4 versos octos√≠labos con rima en pares.
* **Eleg√≠a**: Poema melanc√≥lico sobre la p√©rdida.
* **√âgloga**: Di√°logo buc√≥lico entre pastores.
* **Lira**: Estrofa de 5 versos con m√©trica 7-11-7-7-11.
* **Redondilla**: Estrofa de 4 versos octos√≠labos con rima ABBA.
""")