
import os
import pandas as pd
import streamlit as st
import requests
import traceback

# =========================
# CONFIGURACI√ìN DE LA P√ÅGINA Y ESTILOS
# =========================
st.set_page_config(
    page_title="Generador de Poemas con IA",
    page_icon="‚úçÔ∏è",
    layout="wide",
)

# =========================
# DIAGN√ìSTICO Y CARGA INICIAL
# =========================

csv_path = "poems_clean.csv"
df = None
try:
    df = pd.read_csv(csv_path)
except Exception:
    st.sidebar.error("Error: No se pudo cargar poems_clean.csv. Verifica que est√© en la ra√≠z.")
    df = None

# Carga del token desde entorno o Secrets (Streamlit Cloud)
HF_TOKEN = os.getenv("HF_TOKEN") or st.secrets.get("HF_TOKEN")
if not HF_TOKEN:
    st.sidebar.warning("‚ö†Ô∏è HF_TOKEN no encontrado. Config√∫ralo como variable de entorno o en st.secrets['HF_TOKEN'].")

# =========================
# CONFIGURACI√ìN DEL MODELO Y API
# =========================
# Usa la Inference API p√∫blica (NO el router). Base URL correcta:
# https://api-inference.huggingface.co/models/{model_id}
DEFAULT_MODEL_ID = "HuggingFaceH4/zephyr-7b-beta"
FALLBACK_MODEL_ID = "gpt2"  # en caso de 404 u otros errores del modelo

def inference_api_url(model_id: str) -> str:
    return f"https://api-inference.huggingface.co/models/{model_id}"

def hf_generate(prompt, model_id=DEFAULT_MODEL_ID, max_tokens=300, temperature=0.9, return_full_text=False):
    """Cliente HTTP para Hugging Face Inference API con manejo de errores y fallback."""
    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": max_tokens,
            "temperature": temperature,
            # par√°metros de pipelines se pasan v√≠a "parameters"
            # para text-generation puedes usar return_full_text=False para no repetir el prompt
            "return_full_text": return_full_text,
            # opcionales: top_p, top_k, repetition_penalty...
        }
    }

    try:
        resp = requests.post(inference_api_url(model_id), headers=headers, json=payload, timeout=180)
        resp.raise_for_status()
        data = resp.json()

        # La API puede devolver lista o dict; ambos incluyen 'generated_text'
        if isinstance(data, list) and data and "generated_text" in data[0]:
            return data[0]["generated_text"], model_id
        elif isinstance(data, dict) and "generated_text" in data:
            return data["generated_text"], model_id

        # Algunas implementaciones devuelven objetos m√°s ricos; intenta extraer texto
        if isinstance(data, list) and data and isinstance(data[0], dict):
            # busca cualquier campo parecido
            for k in ("generated_text", "text", "output_text"):
                if k in data[0]:
                    return data[0][k], model_id

        return "Error: Respuesta inesperada de la API.", model_id

    except requests.HTTPError as e:
        status_code = e.response.status_code

        # 404: el modelo no est√° disponible en la Inference API p√∫blica -> intenta fallback
        if status_code == 404 and model_id != FALLBACK_MODEL_ID:
            st.info(f"‚ÑπÔ∏è 404 con {model_id}. Cambiando a modelo de respaldo: {FALLBACK_MODEL_ID}.")
            return hf_generate(prompt, model_id=FALLBACK_MODEL_ID, max_tokens=max_tokens,
                               temperature=temperature, return_full_text=return_full_text)

        # 503: Cold start o servicio no disponible -> muestra error claro
        if status_code == 503:
            return "üíî **Error 503: Servicio no disponible.** El modelo est√° cargando o no acepta tr√°fico ahora.", model_id

        # Otros errores HTTP
        return f"üö® Error HTTP de Hugging Face: {status_code} - {e.response.text}", model_id

    except requests.exceptions.Timeout:
        return "‚è∞ **Timeout**: el modelo tard√≥ demasiado en responder.", model_id
    except Exception as e:
        return "üö® Error inesperado durante la generaci√≥n.\n" + "".join(traceback.format_exception(e)), model_id

# =========================
# INTERFAZ STREAMLIT
# =========================

st.title("‚úçÔ∏è IA Generativa de Poemas en Espa√±ol")

# Selector de modelo (opcional) para que puedas alternar r√°pidamente
model_choice = st.sidebar.selectbox(
    "Modelo (Inference API)",
    options=[DEFAULT_MODEL_ID, "meta-llama/Llama-2-7b-chat-hf", FALLBACK_MODEL_ID],
    index=0,
    help="Si eliges Llama 2, aseg√∫rate de aceptar su licencia en Hugging Face y tener el token con permisos."
)

st.markdown(f"""
Aplicaci√≥n generativa de poemas en espa√±ol usando el modelo **{model_choice}** (v√≠a Hugging Face Inference API).
""")

st.subheader("Configuraci√≥n de la Generaci√≥n")

col1, col2 = st.columns(2)

with col1:
    tema = st.text_input("Tema del poema", placeholder="Ej: La melancol√≠a del oto√±o")

with col2:
    estilo = st.selectbox(
        "Estilo",
        ["Verso libre", "Soneto", "Haiku", "Romance", "D√©cima", "Oda",
         "Copla", "Eleg√≠a", "√âgloga", "Lira", "Redondilla"]
    )

if st.button("‚ú® Generar Poema", type="primary"):
    if not tema or len(tema.strip()) < 3:
        st.error("Por favor, ingresa un tema v√°lido para la generaci√≥n.")
    elif not HF_TOKEN:
        st.error("El token de Hugging Face (HF_TOKEN) es necesario.")
    elif df is None or df.empty:
        st.error("El dataset de poemas no se carg√≥ correctamente.")
    else:
        # 1. Preparar Ejemplos y Prompt
        ejemplos = df['content'].dropna().sample(min(3, len(df))).tolist()
        ejemplos_texto = "\n".join([f"- {e.strip()[:200]}..." for e in ejemplos])

        prompt = f"""
Eres un poeta experto en espa√±ol.
Escribe un poema sobre el tema: "{tema}".
Estilo: {estilo}.
Insp√≠rate en el estilo (sin copiar) de estos ejemplos:
{ejemplos_texto}

Ahora escribe el poema:
""".strip()

        # 2. Generar el Poema con Feedback Visual (Spinner)
        st.subheader(f"Resultado: Poema '{estilo}' sobre '{tema}'")
        with st.spinner("‚è≥ La IA est√° escribiendo... Esto puede tardar varios segundos."):
            poem, used_model = hf_generate(prompt, model_id=model_choice, max_tokens=300, temperature=0.9, return_full_text=False)

        # 3. Mostrar resultado
        st.success(f"‚úÖ Generaci√≥n completada con **{used_model}**.")
        st.markdown("---")
        st.markdown(poem)
        st.markdown("---")

st.markdown("""
---
### Estilos Disponibles:
* **Verso libre**: Poema sin rima ni m√©trica fija.
* **Soneto**: 14 versos endecas√≠labos con rima organizada.
* **Haiku**: Tres versos breves inspirados en la naturaleza.
* **Romance**: Versos octos√≠labos con rima asonante en pares.
* **D√©cima**: 10 versos octos√≠labos con rima ABBAACCDDC.
* **Oda**: Poema solemne y reflexivo.
* **Copla**: Estrofa de 4 versos octos√≠labos con rima en pares.
* **Eleg√≠a**: Poema melanc√≥lico sobre la p√©rdida.
* **√âgloga**: Di√°logo buc√≥lico entre pastores.
* **Lira**: Estrofa de 5 versos con m√©trica 7-11-7-7-11.
* **Redondilla**: Estrofa de 4 versos octos√≠labos con rima ABBA.
""")